# TEN Agent - Voice & Text AI Agent

A comprehensive AI agent that combines voice recognition, natural language processing, and text-to-speech capabilities using modern open-source tools.

## ğŸ¯ Overview

TEN Agent is a modular AI agent that provides:
- **Real-time voice communication** via WebRTC (LiveKit)
- **Speech-to-text** using OpenAI Whisper
- **Intelligent responses** using Ollama LLM
- **Natural speech synthesis** using Piper TTS

## ğŸš€ Quick Start

### Prerequisites
- Node.js 16+
- Python 3.8+
- macOS/Linux (Windows WSL2 supported)

### Installation

1. **Clone and setup:**
```bash
git clone <repository>
cd ten-agent
npm run setup
```

2. **Start services:**
```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start LiveKit (in dev mode)
livekit-server --dev

# Terminal 3: Start TEN Agent
npm start
```

3. **Access playground:**
- Open http://localhost:3000/playground.html
- Test voice and text interactions

## ğŸ—ï¸ Architecture

### Core Components
- **LiveKit RTC**: WebRTC for real-time audio/video
- **Whisper STT**: Speech-to-text transcription
- **Ollama LLM**: Local language model inference
- **Piper TTS**: High-quality text-to-speech

### Extension System
Each extension is self-contained with:
- `manifest.json`: Extension metadata
- `extension.py`: Core functionality
- Modular design for easy addition/removal

## ğŸ“ Project Structure

```
ten-agent/
â”œâ”€â”€ api/                    # REST API server
â”‚   â””â”€â”€ server.js          # Express.js API endpoints
â”œâ”€â”€ extensions/            # AI extensions
â”‚   â”œâ”€â”€ livekit_rtc/       # WebRTC communication
â”‚   â”œâ”€â”€ whisper_stt/       # Speech-to-text
â”‚   â”œâ”€â”€ ollama_llm/        # Language model
â”‚   â””â”€â”€ piper_tts/         # Text-to-speech
â”œâ”€â”€ public/               # Web interface
â”‚   â”œâ”€â”€ playground.html   # Interactive testing UI
â”‚   â””â”€â”€ index.html        # Landing page
â”œâ”€â”€ scripts/              # Setup and utilities
â”‚   â”œâ”€â”€ setup.sh          # Automated setup
â”‚   â””â”€â”€ test.js           # System tests
â”œâ”€â”€ uploads/              # Temporary files
â””â”€â”€ logs/                 # Application logs
```

## ğŸ”§ Configuration

### Environment Variables
Create `.env` from `.env.template`:
```bash
cp .env.template .env
```

Key settings:
- `OLLAMA_MODEL`: Default LLM model (default: llama2)
- `LIVEKIT_URL`: LiveKit server URL
- `WHISPER_MODEL`: Whisper model size (tiny/base/small/medium/large)
- `PIPER_VOICE`: Piper voice model

### Extension Configuration
Each extension can be configured via:
1. Environment variables
2. Extension-specific config files
3. API parameters

## ğŸ§ª Testing

### System Tests
```bash
# Run comprehensive tests
npm test

# Check individual components
node scripts/test.js
```

### Manual Testing
1. **Web Interface**: Use playground.html
2. **API Testing**: Use curl or Postman
3. **Extension Testing**: Run individual extension tests

## ğŸ® Usage Examples

### Voice Interaction
```javascript
// Start recording
POST /api/transcribe
// Audio file â†’ Text

// Get response
POST /api/chat
// Text â†’ LLM response + Audio
```

### Text Interaction
```javascript
// Direct chat
POST /api/chat
{ "message": "Hello, how are you?" }
```

### WebRTC Session
```javascript
// Join room
POST /api/livekit/join
{ "room": "test-room", "identity": "user-123" }
```

## ğŸ” Troubleshooting

### Common Issues

**Ollama not responding:**
```bash
# Check if running
curl http://localhost:11434/api/tags

# Start manually
ollama serve
```

**Whisper import error:**
```bash
# Reinstall whisper
pip3 install openai-whisper --force-reinstall
```

**LiveKit connection failed:**
```bash
# Check if running
curl http://localhost:7880

# Start in dev mode
livekit-server --dev
```

### Debug Mode
Enable detailed logging:
```bash
DEBUG=1 npm start
```

## ğŸ“Š Performance

### Resource Requirements
- **Minimum**: 4GB RAM, 2 CPU cores
- **Recommended**: 8GB RAM, 4 CPU cores
- **GPU**: Optional (CUDA support for Whisper/Ollama)

### Optimization Tips
1. Use smaller Whisper models for faster transcription
2. Adjust Ollama model size based on hardware
3. Enable GPU acceleration where available

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Add tests for new extensions
4. Submit pull request

### Extension Development
Create new extensions:
1. Add folder in `extensions/`
2. Create `manifest.json` and `extension.py`
3. Update API endpoints in `api/server.js`
4. Add tests in `scripts/test.js`

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ†˜ Support

- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions
- **Documentation**: Wiki pages

## ğŸ”„ Updates

Stay updated:
```bash
git pull origin main
npm update
npm run setup  # Re-run for new dependencies